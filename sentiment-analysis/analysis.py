# -*- coding: utf-8 -*-
"""Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l2wGX8sTX-yTx-xdYnUWbN2j0CVaFNuQ
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.optimizers import SGD, Adam, RMSprop
from tensorflow.keras.losses import SparseCategoricalCrossentropy, MeanSquaredError
from tensorflow.keras.callbacks import EarlyStopping

from google.colab import files
uploaded = files.upload()

data=pd.read_csv("IMDB Dataset.csv")

df = pd.DataFrame(data)

# Preprocessing
texts = df['review'].values
labels = df['sentiment'].values

# Tokenize the text
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(texts)
X = tokenizer.texts_to_sequences(texts)
X = pad_sequences(X, padding='post', maxlen=100)  # Pad sequences for uniform length

# Convert labels to numpy array
y = np.array(labels)

from google.colab import files
uploaded = files.upload()

# Word Embeddings (GloVe or Word2Vec)
# Assume you have pre-trained embeddings, e.g., GloVe (50-dimensional vectors)
embedding_dim = 50
embeddings_index = {}
with open('glove.6B.50d.txt', 'r', encoding='utf-8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs

# Prepare embedding matrix
embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))
for word, i in tokenizer.word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector

print(df['sentiment'].unique())
print(df['sentiment'].isnull().sum())

# Remove rows with unexpected or missing labels
df = df[df['sentiment'].isin(['positive', 'negative'])]

# Now map to integers
df['sentiment'] = df['sentiment'].map({'negative': 0, 'positive': 1})

# Drop any rows with unmapped/null values just in case
df = df.dropna(subset=['sentiment'])

# Convert to int (optional but safe)
df['sentiment'] = df['sentiment'].astype(int)

# Then one-hot encode
y_int = df['sentiment'].values
y_cat = to_categorical(y_int, num_classes=2)

# Define Model Architecture
def create_model(activation_function='relu', optimizer='adam', loss_function='binary_crossentropy'):
    model = Sequential()
    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))
    model.add(Flatten())
    model.add(Dense(64, activation=activation_function))

    if loss_function == 'sparse_categorical_crossentropy':
        model.add(Dense(2, activation='softmax'))  # 2 output classes
    else:
        model.add(Dense(1, activation='sigmoid'))  # Binary classification

    model.compile(optimizer=optimizer, loss=loss_function, metrics=['accuracy'])
    return model

from tensorflow.keras.preprocessing.text import Tokenizer

# Set desired number of words (or use None for all)
vocab_size = 10000

tokenizer = Tokenizer(num_words=vocab_size, oov_token="<OOV>")
tokenizer.fit_on_texts(df['review'])  # or whatever your text column is
sequences = tokenizer.texts_to_sequences(df['review'])

# EarlyStopping callback to prevent overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=3)

# Model Training with different configurations
activation_functions = ['relu', 'sigmoid', 'tanh']
optimizers = ['sgd', 'adam', 'rmsprop']
loss_functions = ['sparse_categorical_crossentropy', 'mean_squared_error']

max_length = 100  # or 200, depending on your dataset

from tensorflow.keras.layers import Flatten

# Loop through combinations and train
results = []
for activation in activation_functions:
    for optimizer in optimizers:
        for loss in loss_functions:
            print(f"Training with activation={activation}, optimizer={optimizer}, loss={loss}")
            model = create_model(activation_function=activation, optimizer=optimizer, loss_function=loss)
            model.fit(X, y, epochs=10, batch_size=32, validation_split=0.2, callbacks=[early_stopping], verbose=1)
            val_loss, val_acc = model.evaluate(X, y, verbose=0)
            results.append((activation, optimizer, loss, val_acc))

# Display Results
results_df = pd.DataFrame(results, columns=['Activation', 'Optimizer', 'Loss Function', 'Validation Accuracy'])
print(results_df)

#âœ… Corresponding Configuration:
#Activation: sigmoid

#Optimizer: adam

#Loss Function: sparse_categorical_crossentropy